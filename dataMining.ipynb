{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "# Télécharger les ressources nécessaires pour nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Définir le chemin du dossier parent contenant les dossiers de catégorie\n",
    "chemin_dossier_parent = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\ING2\\\\Projet Sem'\n",
    "\n",
    "# Initialiser une liste pour stocker les données\n",
    "donnees = []\n",
    "\n",
    "# Initialiser le stemmer et le lemmatizer pour l'arabe\n",
    "arabic_stemmer = ISRIStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Parcourir tous les fichiers dans le dossier parent\n",
    "for dossier in os.listdir(chemin_dossier_parent):\n",
    "    # Construire le chemin complet du dossier\n",
    "    chemin_dossier = os.path.join(chemin_dossier_parent, dossier)\n",
    "\n",
    "    # Vérifier si le chemin correspond à un dossier\n",
    "    if os.path.isdir(chemin_dossier):\n",
    "        # Parcourir tous les fichiers dans le dossier\n",
    "        for fichier in os.listdir(chemin_dossier):\n",
    "            # Vérifier si le fichier a l'extension \".txt\"\n",
    "            if fichier.endswith('.txt'):\n",
    "                chemin_fichier = os.path.join(chemin_dossier, fichier)\n",
    "\n",
    "                # Lire seulement 10% du fichier\n",
    "                with open(chemin_fichier, 'r', encoding='utf-8') as file:\n",
    "                    contenu = file.read(int(0.1 * os.path.getsize(chemin_fichier)))\n",
    "\n",
    "                # Supprimer les caractères de nouvelle ligne (\\n) du contenu\n",
    "                contenu = contenu.replace('\\n', ' ')\n",
    "\n",
    "                # Supprimer les chiffres et les ponctuations\n",
    "                contenu = re.sub(r'[\\d' + re.escape(punctuation) + ']', '', contenu)\n",
    "\n",
    "                # Tokenization\n",
    "                tokens = word_tokenize(contenu)\n",
    "\n",
    "                # Supprimer les stopwords\n",
    "                stopwords_arabe = set(stopwords.words('arabic'))\n",
    "                tokens_sans_stopwords = [mot for mot in tokens if mot not in stopwords_arabe]\n",
    "\n",
    "                # Stemming\n",
    "                tokens_stemming = [arabic_stemmer.stem(mot) for mot in tokens_sans_stopwords]\n",
    "\n",
    "                # Lemmatization\n",
    "                tokens_lemmatization = [lemmatizer.lemmatize(mot) for mot in tokens_stemming]\n",
    "\n",
    "                # Ajouter le contenu du fichier, la catégorie, et les tokens à la liste\n",
    "                donnees.append({\n",
    "                    'Catégorie': dossier,\n",
    "                    'Nom du Fichier': fichier,\n",
    "                    'Contenu': contenu,\n",
    "                    'Tokens': tokens,\n",
    "                    'tokens_lemmatization':tokens_lemmatization\n",
    "                })\n",
    "\n",
    "# Créer un dataframe à partir de la liste\n",
    "df = pd.DataFrame(donnees)\n",
    "\n",
    "# Afficher le dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Explorer le DataFrame\n",
    "# Calculer le nombre total de mots dans la colonne 'tokens_sans_mots_frequents'\n",
    "nombre_total_mots = df['tokens_lemmatization'].apply(len).sum()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(f\"Nombre total de mots : {nombre_total_mots}\")\n",
    "# Nombre total de mots dans chaque document\n",
    "df['Nombre de Mots'] = df['tokens_lemmatization'].apply(len)\n",
    "print('Nombre total de mots dans chaque document',df['Nombre de Mots'])\n",
    "\n",
    "# Nombre total de mots par catégorie\n",
    "mots_par_categorie = df.groupby('Catégorie')['Nombre de Mots'].sum()\n",
    "print('Nombre total de mots par catégorie',mots_par_categorie)\n",
    "\n",
    "# Nombre total de mots distincts\n",
    "mots_distincts = set([mot for sublist in df['tokens_lemmatization'] for mot in sublist])\n",
    "nombre_mots_distincts = len(mots_distincts)\n",
    "print('Nombre total de mots distincts',nombre_mots_distincts)\n",
    "# Mots les plus fréquents\n",
    "mots_freq = pd.Series([mot for sublist in df['tokens_lemmatization'] for mot in sublist]).value_counts()\n",
    "print('Mots les plus fréquents',mots_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouper le DataFrame par catégorie\n",
    "groupes_par_categorie = df.groupby('Catégorie')['tokens_lemmatization']\n",
    "\n",
    "# Mots les plus fréquents par catégorie\n",
    "mots_freq_max_par_categorie = groupes_par_categorie.apply(lambda x: pd.Series([mot for sublist in x for mot in sublist]).value_counts().idxmax())\n",
    "mots_freq_max_par_categorie = mots_freq_max_par_categorie.reset_index()\n",
    "mots_freq_max_par_categorie.columns = ['Catégorie', 'Mot le plus fréquent']\n",
    "print('Mots les plus fréquents par catégorie :\\n', mots_freq_max_par_categorie)\n",
    "\n",
    "# Mots les moins fréquents par catégorie\n",
    "mots_freq_min_par_categorie = groupes_par_categorie.apply(lambda x: pd.Series([mot for sublist in x for mot in sublist]).value_counts().idxmin())\n",
    "mots_freq_min_par_categorie = mots_freq_min_par_categorie.reset_index()\n",
    "mots_freq_min_par_categorie.columns = ['Catégorie', 'Mot le moins fréquent']\n",
    "print('\\nMots les moins fréquents par catégorie :\\n', mots_freq_min_par_categorie)\n",
    "\n",
    "# Visualisations\n",
    "# Nombre total de mots par catégorie\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=mots_par_categorie.index, y=mots_par_categorie.values)\n",
    "plt.title('Nombre total de mots par catégorie')\n",
    "plt.xlabel('Catégorie')\n",
    "plt.ylabel('Nombre de Mots')\n",
    "plt.show()\n",
    "\n",
    "# Nombre total de mots distincts\n",
    "print(f\"Nombre total de mots distincts : {nombre_mots_distincts}\")\n",
    "\n",
    "# Mots les plus fréquents\n",
    "top_mots = mots_freq.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_mots.values, y=top_mots.index, orient='h')\n",
    "plt.title('Top 10 des mots les plus fréquents')\n",
    "plt.xlabel('Nombre d\\'Occurrences')\n",
    "plt.ylabel('Mot')\n",
    "plt.show()\n",
    "\n",
    "# Visualisation de mots par catégorie\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Catégorie', y='Nombre de Mots', data=df)\n",
    "plt.title('Distribution du nombre de mots par catégorie')\n",
    "plt.xlabel('Catégorie')\n",
    "plt.ylabel('Nombre de Mots')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Concaténer tous les tokens_lemmatization de toutes les lignes en une seule liste\n",
    "all_tokens = [mot for sublist in df['tokens_lemmatization'] for mot in sublist]\n",
    "\n",
    "# Calculer les fréquences de chaque mot\n",
    "freq_mots = Counter(all_tokens)\n",
    "\n",
    "# Spécifier le seuil de fréquence (par exemple, supprimer les mots qui apparaissent plus de 100 fois)\n",
    "seuil_freq = 1000\n",
    "mots_a_supprimer = [mot for mot, freq in freq_mots.items() if freq > seuil_freq]\n",
    "\n",
    "# Supprimer les mots fréquents de chaque liste de tokens_lemmatization\n",
    "df['tokens_sans_mots_frequents'] = df['tokens_lemmatization'].apply(lambda tokens: [mot for mot in tokens if mot not in mots_a_supprimer])\n",
    "\n",
    "# Afficher le nouveau DataFrame\n",
    "print(df[['Catégorie', 'Nom du Fichier', 'tokens_sans_mots_frequents']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le nombre total de mots dans la colonne 'tokens_sans_mots_frequents'\n",
    "nombre_total_mots = df['tokens_sans_mots_frequents'].apply(len).sum()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(f\"Nombre total de mots dans la colonne 'tokens_sans_mots_frequents' : {nombre_total_mots}\")\n",
    "\n",
    "# Mots les plus fréquents\n",
    "mots_freq = pd.Series([mot for sublist in df['tokens_sans_mots_frequents'] for mot in sublist]).value_counts()\n",
    "print(mots_freq)\n",
    "# Mots les plus fréquents\n",
    "top_mots = mots_freq.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_mots.values, y=top_mots.index, orient='h')\n",
    "plt.title('Top 10 des mots les plus fréquents')\n",
    "plt.xlabel('Nombre d\\'Occurrences')\n",
    "plt.ylabel('Mot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concaténer tous les tokens_sans_mots_frequents de toutes les lignes en une seule liste\n",
    "all_tokens_sans_mots_frequents = [mot for sublist in df['tokens_sans_mots_frequents'] for mot in sublist]\n",
    "\n",
    "# Créer une chaîne de texte à partir de la liste de mots\n",
    "texte = ' '.join(all_tokens_sans_mots_frequents)\n",
    "\n",
    "# Créer un objet WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texte)\n",
    "\n",
    "# Afficher le nuage de mots\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un nouveau DataFrame avec chaque mot de la colonne 'Tokens' comme colonne\n",
    "df_tokens_separes = pd.DataFrame(df['tokens_sans_mots_frequents'].tolist(), columns=[f'Mot_{i}' for i in range(df['tokens_sans_mots_frequents'].apply(len).max())])\n",
    "df_tokens_separes['categorie']=df['Catégorie']\n",
    "\n",
    "# Afficher le nouveau DataFrame\n",
    "print(df_tokens_separes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convertir les NaN en chaînes vides pour éviter les erreurs lors de la vectorisation\n",
    "df_tokens_separes = df_tokens_separes.fillna('')\n",
    "df_tokens=df_tokens_separes.drop('categorie',axis=1)\n",
    "\n",
    "# Convertir chaque ligne du DataFrame en une chaîne de texte\n",
    "texte_documents = df_tokens_separes.apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "# Initialiser le vectoriseur TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Appliquer le TF-IDF sur le DataFrame\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texte_documents)\n",
    "\n",
    "# Créer un nouveau DataFrame avec les résultats de la vectorisation TF-IDF\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Afficher le nouveau DataFrame TF-IDF\n",
    "print(df_tfidf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'attributs TF-IDF\n",
    "nombre_attributs_tfidf = df_tfidf.shape[1]\n",
    "print(\"Nombre d'attributs TF-IDF :\", nombre_attributs_tfidf)\n",
    "\n",
    "# Pourcentage de zéros dans la matrice TF-IDF\n",
    "pourcentage_zeros_tfidf = (df_tfidf == 0).sum().sum() / (df_tfidf.shape[0] * df_tfidf.shape[1]) * 100\n",
    "print(\"Pourcentage de zéros dans la matrice TF-IDF :\", pourcentage_zeros_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Appliquer PCA avec 50 composantes principales\n",
    "nombre_composantes = 50\n",
    "pca = PCA(n_components=nombre_composantes)\n",
    "pca_resultats = pca.fit_transform(df_tfidf)\n",
    "\n",
    "# Créer un DataFrame avec les résultats de la PCA\n",
    "df_pca = pd.DataFrame(data=pca_resultats, columns=[f'PC{i}' for i in range(1, nombre_composantes + 1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le nouveau DataFrame PCA\n",
    "print(df_pca.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Spécifier le nombre de clusters (k)\n",
    "nombre_clusters = 6\n",
    "\n",
    "# Initialiser le modèle K-means\n",
    "kmeans = KMeans(n_clusters=nombre_clusters, random_state=42)\n",
    "\n",
    "clusters_predits = kmeans.fit_predict(df_pca)\n",
    "\n",
    "# Calculer l'indice de silhouette\n",
    "silhouette_avg = silhouette_score(df_pca, clusters_predits)\n",
    "\n",
    "print(f'Indice de silhouette : {silhouette_avg}')\n",
    "\n",
    "\n",
    "# Ajouter la colonne des clusters à la dataframe\n",
    "df_pca['Cluster'] = clusters_predits\n",
    "\n",
    "# Afficher le résultat\n",
    "print(df_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "# Spécifier le nombre de clusters (k)\n",
    "nombre_clusters = 6\n",
    "\n",
    "# Initialiser le modèle K-means\n",
    "kmeans = KMeans(n_clusters=nombre_clusters, random_state=42)\n",
    "\n",
    "clusters_predits = kmeans.fit_predict(df_pca)\n",
    "\n",
    "# Calculer l'indice de silhouette\n",
    "silhouette_avg = silhouette_score(df_pca, clusters_predits)\n",
    "\n",
    "print(f'Indice de silhouette : {silhouette_avg}')\n",
    "\n",
    "# Ajouter la colonne des clusters à la dataframe\n",
    "df_pca['Cluster'] = clusters_predits\n",
    "\n",
    "# Afficher le résultat\n",
    "print(df_pca.head())\n",
    "\n",
    "# Visualisation des clusters (2D - choisissez deux composantes principales)\n",
    "scatter = plt.scatter(df_pca['PC1'], df_pca['PC2'], c=clusters_predits, cmap='viridis')\n",
    "plt.title('Clusters K-means (2D)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "legend_labels = [f'Cluster {i}' for i in range(nombre_clusters)]\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels, title='Clusters')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la colonne des clusters à la dataframe\n",
    "df_pca['Cluster'] = clusters_predits\n",
    "\n",
    "# Créer un DataFrame pour analyser les caractéristiques des clusters\n",
    "df_cluster_analysis = pd.DataFrame({\n",
    "    'Cluster': clusters_predits,\n",
    "    'Categorie': df_tokens_separes['categorie']  # Assurez-vous que le nom de la colonne est correct\n",
    "})\n",
    "\n",
    "# Examiner la répartition des catégories dans chaque cluster\n",
    "cluster_category_distribution = df_cluster_analysis.groupby(['Cluster', 'Categorie']).size().reset_index(name='Counts')\n",
    "#print(cluster_category_distribution)\n",
    "\n",
    "# Pour chaque cluster, obtenir la catégorie dominante\n",
    "dominant_category_in_cluster = df_cluster_analysis.groupby('Cluster')['Categorie'].agg(lambda x: x.value_counts().idxmax()).reset_index(name='Dominant_Category')\n",
    "print(dominant_category_in_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ajouter la colonne des clusters à la dataframe\n",
    "df_pca['Cluster'] = clusters_predits\n",
    "\n",
    "# Initialiser une liste pour stocker les tokens_sans_mots_frequents par cluster\n",
    "all_tokens_sans_mots_frequents_par_cluster = []\n",
    "\n",
    "# Concaténer tous les tokens_sans_mots_frequents de toutes les lignes en une seule liste\n",
    "for cluster in range(nombre_clusters):\n",
    "    mots_cluster = [mot for sublist in df[df_pca['Cluster'] == cluster]['tokens_sans_mots_frequents'] for mot in sublist]\n",
    "    all_tokens_sans_mots_frequents_par_cluster.append(mots_cluster)\n",
    "\n",
    "# Créer une chaîne de texte pour chaque cluster\n",
    "texte_par_cluster = [' '.join(mots_cluster) for mots_cluster in all_tokens_sans_mots_frequents_par_cluster]\n",
    "\n",
    "# Créer un nuage de mots pour chaque cluster\n",
    "for cluster, texte_cluster in enumerate(texte_par_cluster):\n",
    "    # Créer un nuage de mots pour le cluster actuel\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texte_cluster)\n",
    "\n",
    "    # Afficher le nuage de mots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Nuage de mots - Cluster {cluster}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nuage de mot par paire de cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_pca is your DataFrame with PCA results and Cluster column\n",
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(df_pca, hue=\"Cluster\", palette=\"husl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convertir les NaN en chaînes vides pour éviter les erreurs lors de la vectorisation\n",
    "df_tokens_separes = df_tokens_separes.fillna('')\n",
    "df_tokens=df_tokens_separes.drop('categorie',axis=1)\n",
    "# Convertir chaque ligne du DataFrame en une chaîne de texte\n",
    "texte_documents =df_tokens.apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "# Initialiser le vectoriseur TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Appliquer le TF-IDF sur le DataFrame\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texte_documents)\n",
    "\n",
    "# Créer un nouveau DataFrame avec les résultats de la vectorisation TF-IDF\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Afficher le nouveau DataFrame TF-IDF\n",
    "print(df_tfidf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Spécifiez le nombre de thèmes à extraire (nombre de clusters)\n",
    "nombre_themes = 6\n",
    "\n",
    "# Initialiser le modèle NMF\n",
    "nmf = NMF(n_components=nombre_themes, random_state=42)\n",
    "\n",
    "# Appliquer le modèle NMF sur les données TF-IDF\n",
    "themes_predits = nmf.fit_transform(df_tfidf)\n",
    "\n",
    "# Ajouter les colonnes des thèmes à la dataframe\n",
    "for i in range(nombre_themes):\n",
    "    df[f'Theme_{i}'] = themes_predits[:, i]\n",
    "\n",
    "\n",
    "# Vous pouvez également explorer les termes les plus importants par thème\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(nmf.components_):\n",
    "    print(f\"Theme {i}:\")\n",
    "    print(\" \".join([feature_names[idx] for idx in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Calculer la divergence KL entre les distributions de probabilité des termes\n",
    "kl_divergences = []\n",
    "for i, topic in enumerate(nmf.components_):\n",
    "    kl_divergence = np.sum(topic * np.log(topic / df_tfidf.sum(axis=0)))\n",
    "    kl_divergences.append(kl_divergence)\n",
    "\n",
    "# Afficher les divergences KL par thème\n",
    "for i, kl_divergence in enumerate(kl_divergences):\n",
    "    print(f\"Theme {i}: KL Divergence = {kl_divergence}\")\n",
    "\n",
    "# Évaluer l'interprétabilité des thèmes\n",
    "for i, topic in enumerate(nmf.components_):\n",
    "    top_terms = [feature_names[idx] for idx in topic.argsort()[:-10 - 1:-1]]\n",
    "    print(f\"Theme {i}: Top Terms = {', '.join(top_terms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf, df['Catégorie'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser le modèle KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # Vous pouvez ajuster le nombre de voisins (n_neighbors) selon vos besoins\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes de classe sur l'ensemble de test\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculer l'exactitude du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Précision du modèle KNN : {accuracy}\")\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(\"Rapport de Classification :\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Définir le modèle KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # Vous pouvez ajuster le nombre de voisins (n_neighbors) selon vos besoins\n",
    "\n",
    "# Définir les caractéristiques (X) et les étiquettes (y)\n",
    "X = df_tfidf\n",
    "y = df['Catégorie']\n",
    "\n",
    "# Définir la stratégie de validation croisée\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Effectuer la validation croisée\n",
    "scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Afficher les scores pour chaque itération\n",
    "print(\"Scores de validation croisée :\", scores)\n",
    "\n",
    "# Afficher la moyenne des scores\n",
    "print(\"Moyenne des scores de validation croisée :\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Définir les caractéristiques (X) et les étiquettes (y)\n",
    "X = df['tokens_sans_mots_frequents']\n",
    "y = df['Catégorie']\n",
    "\n",
    "X = [' '.join(tokens) for tokens in df['tokens_sans_mots_frequents']]\n",
    "\n",
    "# Vectoriser les données textuelles en utilisant TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir le modèle KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Définir la grille d'hyperparamètres à tester\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']}\n",
    "\n",
    "# Initialiser la recherche sur grille\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Effectuer la recherche sur grille\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres :\", grid_search.best_params_)\n",
    "\n",
    "# Afficher la meilleure précision\n",
    "print(\"Meilleure précision :\", grid_search.best_score_)\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "meilleur_modele = grid_search.best_estimator_\n",
    "precision_test = meilleur_modele.score(X_test, y_test)\n",
    "print(\"Précision sur l'ensemble de test :\", precision_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Définir les caractéristiques (X) et les étiquettes (y)\n",
    "X = df['tokens_sans_mots_frequents']\n",
    "y = df['Catégorie']\n",
    "\n",
    "X = [' '.join(tokens) for tokens in df['tokens_sans_mots_frequents']]\n",
    "\n",
    "# Encoder les étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer et séquençage des textes\n",
    "max_words = 10000  # Nombre maximum de mots à considérer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Paddage des séquences pour avoir une longueur uniforme\n",
    "max_sequence_length = 200  # Longueur maximale des séquences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Construire le modèle de réseau de neurones\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "accuracy_test = model.evaluate(X_test_padded, y_test)[1]\n",
    "print(\"Précision sur l'ensemble de test :\", accuracy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Définir les caractéristiques (X) et les étiquettes (y)\n",
    "X = df_tfidf\n",
    "y = df['Catégorie']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir le modèle Decision Tree\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Évaluer la précision du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision du modèle Decision Tree :\", accuracy)\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Rapport de Classification :\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# Exporter le texte de l'arbre (pour une visualisation textuelle)\n",
    "tree_rules = export_text(decision_tree, feature_names=list(X.columns))\n",
    "print(\"Règles de l'arbre de décision :\\n\", tree_rules)\n",
    "\n",
    "# Exporter l'arbre au format DOT (Graphviz)\n",
    "dot_data = export_graphviz(\n",
    "    decision_tree,\n",
    "    out_file=None,\n",
    "    feature_names=list(X.columns),\n",
    "    class_names=decision_tree.classes_,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True\n",
    ")\n",
    "\n",
    "# Visualiser l'arbre avec Graphviz\n",
    "graph = graphviz.Source(dot_data, filename=\"path/to/dot\", format=\"png\")\n",
    "graph.render(\"decision_tree\", format=\"png\", cleanup=True)\n",
    "graph.view(\"decision_tree\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
